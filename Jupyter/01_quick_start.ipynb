{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Caption Generator - Quick Start\n",
    "\n",
    "This notebook provides an interactive quick start guide to the image captioning project.\n",
    "\n",
    "**What you'll do:**\n",
    "1. Setup environment\n",
    "2. Download and explore data\n",
    "3. Train a simple model\n",
    "4. Generate captions\n",
    "5. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.11/site-packages (0.24.1)\n",
      "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.11/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.11/site-packages (0.12.2)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.11/site-packages (10.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (4.65.0)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install other required packages\n",
    "!pip install pandas numpy matplotlib seaborn pillow tqdm nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in /opt/anaconda3/lib/python3.11/site-packages (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade typing_extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.7 (main, Dec 15 2023, 12:09:56) [Clang 14.0.6 ]\n",
      "PyTorch version: 2.9.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Dataset\n",
    "\n",
    "**Option A:** Run this if you have Kaggle API setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/bahar/Caption_Gen/Jupyter\n",
      "Changed to: /Users/bahar/Caption_Gen\n",
      "Downloading Flickr8k dataset from Kaggle...\n",
      "Make sure you have kaggle.json in ~/.kaggle/\n",
      "Get your API token from: https://www.kaggle.com/settings/account\n",
      "\n",
      "\n",
      "âœ— Kaggle CLI not found. Install it with:\n",
      "pip install kaggle\n",
      "\n",
      "Or download manually from:\n",
      "https://www.kaggle.com/datasets/adityajn105/flickr8k\n"
     ]
    }
   ],
   "source": [
    "# Download Flickr8k dataset\n",
    "import os\n",
    "\n",
    "# Save current directory\n",
    "notebook_dir = os.getcwd()\n",
    "print(f\"Current directory: {notebook_dir}\")\n",
    "\n",
    "# Go up to CAPTION_GEN folder\n",
    "os.chdir('..')\n",
    "print(f\"Changed to: {os.getcwd()}\")\n",
    "\n",
    "# Run download script\n",
    "!python data/download_dataset.py\n",
    "\n",
    "# Go back to Jupyter folder\n",
    "os.chdir(notebook_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option B:** Manual download\n",
    "\n",
    "If Kaggle API doesn't work:\n",
    "1. Go to https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
    "2. Download and extract to `data/flickr8k/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset exists\n",
    "data_dir = Path('../data/flickr8k')\n",
    "images_dir = data_dir / 'Images'\n",
    "captions_file = data_dir / 'captions.txt'\n",
    "\n",
    "if captions_file.exists():\n",
    "    num_images = len(list(images_dir.glob('*.jpg')))\n",
    "    print(f\"âœ“ Dataset found!\")\n",
    "    print(f\"  Images: {num_images}\")\n",
    "    print(f\"  Location: {data_dir}\")\n",
    "else:\n",
    "    print(\"âœ— Dataset not found. Please download it first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "os.chdir('..')  # Go to CAPTION_GEN\n",
    "\n",
    "!python data/preprocess.py\n",
    "\n",
    "os.chdir(notebook_dir)  # Return to Jupyter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed captions\n",
    "df = pd.read_csv('../data/flickr8k/processed/captions_clean.csv')\n",
    "\n",
    "print(f\"Total captions: {len(df):,}\")\n",
    "print(f\"Unique images: {df['image'].nunique():,}\")\n",
    "print(f\"Captions per image: {len(df) / df['image'].nunique():.1f}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption length distribution\n",
    "df['length'] = df['caption'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['length'], bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(df['length'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"length\"].mean():.1f}')\n",
    "plt.xlabel('Caption Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Caption Lengths')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['length'].plot(kind='box', vert=False)\n",
    "plt.xlabel('Caption Length (words)')\n",
    "plt.title('Caption Length Box Plot')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Statistics:\")\n",
    "print(f\"  Mean: {df['length'].mean():.2f} words\")\n",
    "print(f\"  Median: {df['length'].median():.0f} words\")\n",
    "print(f\"  Min: {df['length'].min()} words\")\n",
    "print(f\"  Max: {df['length'].max()} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample images with captions\n",
    "import random\n",
    "\n",
    "sample_images = df['image'].unique()[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_name in enumerate(sample_images):\n",
    "    # Load image\n",
    "    img_path = images_dir / img_name\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    # Get captions\n",
    "    captions = df[df['image'] == img_name]['caption'].values\n",
    "    \n",
    "    # Display\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f\"{img_name}\\n{captions[0][:50]}...\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model (Small Demo)\n",
    "\n",
    "**Note:** For full training, use the Python script:\n",
    "```bash\n",
    "python training/train.py --model cnn_lstm --epochs 20\n",
    "```\n",
    "\n",
    "Here we'll train for just 2 epochs to demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "os.chdir('..')  # Go to CAPTION_GEN\n",
    "\n",
    "!python training/train.py \\\n",
    "  --model cnn_lstm \\\n",
    "  --epochs 30 \\\n",
    "  --batch_size 32 \\\n",
    "  --learning_rate 0.0001 \\\n",
    "  --experiment_name jupyter_training\n",
    "\n",
    "os.chdir(notebook_dir)  # Return to Jupyter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Captions\n",
    "\n",
    "Load a trained model and generate captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "sys.path.append('../')\n",
    "from models.cnn_lstm import CNNLSTMModel\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load checkpoint (use your best model)\n",
    "checkpoint_path = '../checkpoints/cnn_lstm_best.pth'\n",
    "\n",
    "if Path(checkpoint_path).exists():\n",
    "    model, vocab, epoch, loss = CNNLSTMModel.load_from_checkpoint(checkpoint_path, device)\n",
    "    print(f\"âœ“ Model loaded from epoch {epoch}\")\n",
    "    print(f\"  Validation loss: {loss:.4f}\")\n",
    "    print(f\"  Vocabulary size: {len(vocab)}\")\n",
    "else:\n",
    "    print(\"âœ— No trained model found. Please train first.\")\n",
    "    print(\"  Run: python training/train.py --model cnn_lstm --epochs 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def generate_caption_for_image(image_path, beam_size=3):\n",
    "    \"\"\"Generate caption for an image\"\"\"\n",
    "    # Load and preprocess\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate caption\n",
    "    caption = model.generate_caption(image_tensor, vocab, max_length=40, beam_size=beam_size)\n",
    "    \n",
    "    return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate captions for sample images\n",
    "test_images = list(images_dir.glob('*.jpg'))[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_path in enumerate(test_images):\n",
    "    image, caption = generate_caption_for_image(img_path)\n",
    "    \n",
    "    axes[idx].imshow(image)\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f\"Generated: {caption}\", fontsize=10, wrap=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare with Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare generated vs reference captions\n",
    "sample_img = random.choice(list(images_dir.glob('*.jpg')))\n",
    "img_name = sample_img.name\n",
    "\n",
    "# Get reference captions\n",
    "reference_captions = df[df['image'] == img_name]['caption'].values\n",
    "\n",
    "# Generate caption\n",
    "image, generated_caption = generate_caption_for_image(sample_img, beam_size=5)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Image: {img_name}\", fontsize=12, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATED CAPTION:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"â†’ {generated_caption}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REFERENCE CAPTIONS:\")\n",
    "print(\"=\"*70)\n",
    "for i, ref in enumerate(reference_captions, 1):\n",
    "    print(f\"{i}. {ref}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "Now that you've completed the quick start:\n",
    "\n",
    "1. **Full Training:** Train for 20+ epochs using Python script\n",
    "2. **Evaluation:** Check `02_model_evaluation.ipynb`\n",
    "3. **Explainability:** See `03_explainability.ipynb` for Grad-CAM\n",
    "4. **Analysis:** Explore `04_error_analysis.ipynb`\n",
    "5. **Presentation:** Use `05_presentation.ipynb` for final results\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your project! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
