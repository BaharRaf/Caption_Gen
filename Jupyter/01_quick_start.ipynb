{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Caption Generator - Quick Start\n",
    "\n",
    "This notebook provides an interactive quick start guide to the image captioning project.\n",
    "\n",
    "**What you'll do:**\n",
    "1. Setup environment\n",
    "2. Download and explore data\n",
    "3. Train a simple model\n",
    "4. Generate captions\n",
    "5. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Dataset\n",
    "\n",
    "**Option A:** Run this if you have Kaggle API setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download Flickr8k dataset\n",
    "!python ../data/download_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option B:** Manual download\n",
    "\n",
    "If Kaggle API doesn't work:\n",
    "1. Go to https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
    "2. Download and extract to `data/flickr8k/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if dataset exists\n",
    "data_dir = Path('../data/flickr8k')\n",
    "images_dir = data_dir / 'Images'\n",
    "captions_file = data_dir / 'captions.txt'\n",
    "\n",
    "if captions_file.exists():\n",
    "    num_images = len(list(images_dir.glob('*.jpg')))\n",
    "    print(f\"âœ“ Dataset found!\")\n",
    "    print(f\"  Images: {num_images}\")\n",
    "    print(f\"  Location: {data_dir}\")\n",
    "else:\n",
    "    print(\"âœ— Dataset not found. Please download it first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run preprocessing\n",
    "!python ../data/preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load processed captions\n",
    "df = pd.read_csv('../data/flickr8k/processed/captions_clean.csv')\n",
    "\n",
    "print(f\"Total captions: {len(df):,}\")\n",
    "print(f\"Unique images: {df['image'].nunique():,}\")\n",
    "print(f\"Captions per image: {len(df) / df['image'].nunique():.1f}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Caption length distribution\n",
    "df['length'] = df['caption'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['length'], bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(df['length'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"length\"].mean():.1f}')\n",
    "plt.xlabel('Caption Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Caption Lengths')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['length'].plot(kind='box', vert=False)\n",
    "plt.xlabel('Caption Length (words)')\n",
    "plt.title('Caption Length Box Plot')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Statistics:\")\n",
    "print(f\"  Mean: {df['length'].mean():.2f} words\")\n",
    "print(f\"  Median: {df['length'].median():.0f} words\")\n",
    "print(f\"  Min: {df['length'].min()} words\")\n",
    "print(f\"  Max: {df['length'].max()} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Show sample images with captions\n",
    "import random\n",
    "\n",
    "sample_images = df['image'].unique()[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_name in enumerate(sample_images):\n",
    "    # Load image\n",
    "    img_path = images_dir / img_name\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    # Get captions\n",
    "    captions = df[df['image'] == img_name]['caption'].values\n",
    "    \n",
    "    # Display\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f\"{img_name}\\n{captions[0][:50]}...\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model (Small Demo)\n",
    "\n",
    "**Note:** For full training, use the Python script:\n",
    "```bash\n",
    "python training/train.py --model cnn_lstm --epochs 20\n",
    "```\n",
    "\n",
    "Here we'll train for just 2 epochs to demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Quick training demo (2 epochs)\n",
    "!python ../training/train.py --model cnn_lstm --epochs 2 --batch_size 32 --experiment_name demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Captions\n",
    "\n",
    "Load a trained model and generate captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load trained model\n",
    "sys.path.append('../')\n",
    "from models.cnn_lstm import CNNLSTMModel\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load checkpoint (use your best model)\n",
    "checkpoint_path = '../checkpoints/cnn_lstm_best.pth'\n",
    "\n",
    "if Path(checkpoint_path).exists():\n",
    "    model, vocab, epoch, loss = CNNLSTMModel.load_from_checkpoint(checkpoint_path, device)\n",
    "    print(f\"âœ“ Model loaded from epoch {epoch}\")\n",
    "    print(f\"  Validation loss: {loss:.4f}\")\n",
    "    print(f\"  Vocabulary size: {len(vocab)}\")\n",
    "else:\n",
    "    print(\"âœ— No trained model found. Please train first.\")\n",
    "    print(\"  Run: python training/train.py --model cnn_lstm --epochs 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def generate_caption_for_image(image_path, beam_size=3):\n",
    "    \"\"\"Generate caption for an image\"\"\"\n",
    "    # Load and preprocess\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate caption\n",
    "    caption = model.generate_caption(image_tensor, vocab, max_length=40, beam_size=beam_size)\n",
    "    \n",
    "    return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate captions for sample images\n",
    "test_images = list(images_dir.glob('*.jpg'))[:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_path in enumerate(test_images):\n",
    "    image, caption = generate_caption_for_image(img_path)\n",
    "    \n",
    "    axes[idx].imshow(image)\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f\"Generated: {caption}\", fontsize=10, wrap=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare with Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare generated vs reference captions\n",
    "sample_img = random.choice(list(images_dir.glob('*.jpg')))\n",
    "img_name = sample_img.name\n",
    "\n",
    "# Get reference captions\n",
    "reference_captions = df[df['image'] == img_name]['caption'].values\n",
    "\n",
    "# Generate caption\n",
    "image, generated_caption = generate_caption_for_image(sample_img, beam_size=5)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Image: {img_name}\", fontsize=12, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATED CAPTION:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"â†’ {generated_caption}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"REFERENCE CAPTIONS:\")\n",
    "print(\"=\"*70)\n",
    "for i, ref in enumerate(reference_captions, 1):\n",
    "    print(f\"{i}. {ref}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "Now that you've completed the quick start:\n",
    "\n",
    "1. **Full Training:** Train for 20+ epochs using Python script\n",
    "2. **Evaluation:** Check `02_model_evaluation.ipynb`\n",
    "3. **Explainability:** See `03_explainability.ipynb` for Grad-CAM\n",
    "4. **Analysis:** Explore `04_error_analysis.ipynb`\n",
    "5. **Presentation:** Use `05_presentation.ipynb` for final results\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your project! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
