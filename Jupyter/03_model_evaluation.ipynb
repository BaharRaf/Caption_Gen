{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation & Results Analysis\n",
    "\n",
    "Comprehensive evaluation of trained image captioning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from models.cnn_lstm import CNNLSTMModel\n",
    "from data.dataset import get_dataloaders\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "checkpoint_path = '../checkpoints/cnn_lstm_best.pth'\n",
    "model, vocab, epoch, loss = CNNLSTMModel.load_from_checkpoint(checkpoint_path, device)\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully!\")\n",
    "print(f\"  Checkpoint: {checkpoint_path}\")\n",
    "print(f\"  Epoch: {epoch}\")\n",
    "print(f\"  Validation Loss: {loss:.4f}\")\n",
    "print(f\"  Vocabulary Size: {len(vocab):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate on test set\n",
    "!python ../training/evaluate.py \\\n",
    "    --model cnn_lstm \\\n",
    "    --checkpoint ../checkpoints/cnn_lstm_best.pth \\\n",
    "    --beam_size 3 \\\n",
    "    --save_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load evaluation results\n",
    "results_dir = Path('../results')\n",
    "\n",
    "# Load metrics\n",
    "metrics_df = pd.read_csv(results_dir / 'cnn_lstm_metrics.csv')\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(\"=\"*60)\n",
    "print(metrics_df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize metrics\n",
    "metrics = ['bleu1', 'bleu2', 'bleu3', 'bleu4', 'meteor']\n",
    "values = [metrics_df[m].values[0] for m in metrics]\n",
    "labels = ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'METEOR']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c', '#9b59b6']\n",
    "bars = ax1.bar(labels, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Model Performance Metrics', fontweight='bold')\n",
    "ax1.set_ylim(0, 1.0)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# BLEU progression\n",
    "bleu_values = values[:4]\n",
    "bleu_labels = labels[:4]\n",
    "ax2.plot(range(1, 5), bleu_values, marker='o', linewidth=2, markersize=10, color='steelblue')\n",
    "ax2.set_xlabel('N-gram')\n",
    "ax2.set_ylabel('BLEU Score')\n",
    "ax2.set_title('BLEU Score Progression', fontweight='bold')\n",
    "ax2.set_xticks(range(1, 5))\n",
    "ax2.set_xticklabels(bleu_labels)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load predictions\n",
    "predictions_df = pd.read_csv(results_dir / 'cnn_lstm_predictions.csv')\n",
    "\n",
    "print(f\"\\nTotal predictions: {len(predictions_df):,}\")\n",
    "print(f\"\\nFirst 5 predictions:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(5):\n",
    "    print(f\"\\n{i+1}. Reference: {predictions_df.iloc[i]['reference']}\")\n",
    "    print(f\"   Generated: {predictions_df.iloc[i]['hypothesis']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate caption lengths\n",
    "predictions_df['ref_length'] = predictions_df['reference'].apply(lambda x: len(x.split()))\n",
    "predictions_df['hyp_length'] = predictions_df['hypothesis'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Length comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Histogram comparison\n",
    "axes[0].hist(predictions_df['ref_length'], bins=20, alpha=0.5, label='Reference', color='blue', edgecolor='black')\n",
    "axes[0].hist(predictions_df['hyp_length'], bins=20, alpha=0.5, label='Generated', color='red', edgecolor='black')\n",
    "axes[0].set_xlabel('Caption Length (words)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Caption Length Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "axes[1].scatter(predictions_df['ref_length'], predictions_df['hyp_length'], alpha=0.3, s=10)\n",
    "axes[1].plot([0, 30], [0, 30], 'r--', label='Perfect match')\n",
    "axes[1].set_xlabel('Reference Length')\n",
    "axes[1].set_ylabel('Generated Length')\n",
    "axes[1].set_title('Length Correlation')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Box plot comparison\n",
    "axes[2].boxplot([predictions_df['ref_length'], predictions_df['hyp_length']], \n",
    "                labels=['Reference', 'Generated'],\n",
    "                patch_artist=True)\n",
    "axes[2].set_ylabel('Caption Length (words)')\n",
    "axes[2].set_title('Length Statistics')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLength Statistics:\")\n",
    "print(f\"  Reference - Mean: {predictions_df['ref_length'].mean():.2f}, Std: {predictions_df['ref_length'].std():.2f}\")\n",
    "print(f\"  Generated - Mean: {predictions_df['hyp_length'].mean():.2f}, Std: {predictions_df['hyp_length'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Qualitative Analysis - Best Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Calculate per-caption BLEU-4 scores\n",
    "smoothing = SmoothingFunction()\n",
    "predictions_df['bleu4'] = predictions_df.apply(\n",
    "    lambda row: sentence_bleu(\n",
    "        [row['reference'].split()],\n",
    "        row['hypothesis'].split(),\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),\n",
    "        smoothing_function=smoothing.method1\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Get best predictions\n",
    "best_preds = predictions_df.nlargest(10, 'bleu4')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 10 BEST PREDICTIONS (Highest BLEU-4 Scores)\")\n",
    "print(\"=\"*80)\n",
    "for i, (idx, row) in enumerate(best_preds.iterrows(), 1):\n",
    "    print(f\"\\n{i}. BLEU-4: {row['bleu4']:.4f}\")\n",
    "    print(f\"   Reference: {row['reference']}\")\n",
    "    print(f\"   Generated: {row['hypothesis']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Qualitative Analysis - Worst Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get worst predictions\n",
    "worst_preds = predictions_df.nsmallest(10, 'bleu4')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 10 WORST PREDICTIONS (Lowest BLEU-4 Scores)\")\n",
    "print(\"=\"*80)\n",
    "for i, (idx, row) in enumerate(worst_preds.iterrows(), 1):\n",
    "    print(f\"\\n{i}. BLEU-4: {row['bleu4']:.4f}\")\n",
    "    print(f\"   Reference: {row['reference']}\")\n",
    "    print(f\"   Generated: {row['hypothesis']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Distribution of BLEU-4 scores\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(predictions_df['bleu4'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "plt.axvline(predictions_df['bleu4'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {predictions_df[\"bleu4\"].mean():.3f}')\n",
    "plt.xlabel('BLEU-4 Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Per-Caption BLEU-4 Scores')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "predictions_df['bleu4'].plot(kind='box', vert=False)\n",
    "plt.xlabel('BLEU-4 Score')\n",
    "plt.title('BLEU-4 Score Distribution (Box Plot)')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBLEU-4 Statistics:\")\n",
    "print(f\"  Mean: {predictions_df['bleu4'].mean():.4f}\")\n",
    "print(f\"  Median: {predictions_df['bleu4'].median():.4f}\")\n",
    "print(f\"  Std: {predictions_df['bleu4'].std():.4f}\")\n",
    "print(f\"  Min: {predictions_df['bleu4'].min():.4f}\")\n",
    "print(f\"  Max: {predictions_df['bleu4'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Categorize predictions by BLEU-4 score\n",
    "def categorize_score(score):\n",
    "    if score >= 0.5:\n",
    "        return 'Excellent (≥0.5)'\n",
    "    elif score >= 0.3:\n",
    "        return 'Good (0.3-0.5)'\n",
    "    elif score >= 0.15:\n",
    "        return 'Fair (0.15-0.3)'\n",
    "    else:\n",
    "        return 'Poor (<0.15)'\n",
    "\n",
    "predictions_df['category'] = predictions_df['bleu4'].apply(categorize_score)\n",
    "\n",
    "# Count by category\n",
    "category_counts = predictions_df['category'].value_counts()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "category_counts.plot(kind='bar', color=['green', 'blue', 'orange', 'red'], alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Quality Category')\n",
    "plt.ylabel('Number of Predictions')\n",
    "plt.title('Prediction Quality Distribution')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%',\n",
    "       colors=['green', 'blue', 'orange', 'red'], startangle=90)\n",
    "plt.title('Prediction Quality Percentage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPrediction Quality Breakdown:\")\n",
    "print(\"=\"*40)\n",
    "for cat, count in category_counts.items():\n",
    "    pct = count / len(predictions_df) * 100\n",
    "    print(f\"{cat}: {count} ({pct:.1f}%)\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison (If Multiple Models Trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# If you have trained multiple models, load and compare them\n",
    "# Example comparison structure\n",
    "\n",
    "comparison_data = {\n",
    "    'Model': ['CNN-LSTM', 'CNN-Transformer', 'ViT-GPT2'],\n",
    "    'BLEU-4': [0.213, 0.247, 0.281],\n",
    "    'METEOR': [0.198, 0.217, 0.239],\n",
    "    'Training Time (hrs)': [2, 6, 12],\n",
    "    'Parameters (M)': [28, 52, 124]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Metric comparison\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, comparison_df['BLEU-4'], width, label='BLEU-4', alpha=0.7)\n",
    "axes[0].bar(x + width/2, comparison_df['METEOR'], width, label='METEOR', alpha=0.7)\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Performance Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(comparison_df['Model'])\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Performance vs Training Time\n",
    "axes[1].scatter(comparison_df['Training Time (hrs)'], comparison_df['BLEU-4'], \n",
    "               s=comparison_df['Parameters (M)'] * 3, alpha=0.6, c=['blue', 'green', 'red'])\n",
    "axes[1].set_xlabel('Training Time (hours)')\n",
    "axes[1].set_ylabel('BLEU-4 Score')\n",
    "axes[1].set_title('Performance vs Training Time\\n(bubble size = model parameters)')\n",
    "for i, model in enumerate(comparison_df['Model']):\n",
    "    axes[1].annotate(model, \n",
    "                    (comparison_df['Training Time (hrs)'][i], comparison_df['BLEU-4'][i]),\n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nModel: CNN-LSTM (ResNet50 + 2-layer LSTM)\")\n",
    "print(f\"Checkpoint: {checkpoint_path}\")\n",
    "print(f\"Training Epochs: {epoch}\")\n",
    "\n",
    "print(f\"\\n1. Quantitative Results:\")\n",
    "print(f\"   BLEU-1: {metrics_df['bleu1'].values[0]:.4f}\")\n",
    "print(f\"   BLEU-2: {metrics_df['bleu2'].values[0]:.4f}\")\n",
    "print(f\"   BLEU-3: {metrics_df['bleu3'].values[0]:.4f}\")\n",
    "print(f\"   BLEU-4: {metrics_df['bleu4'].values[0]:.4f}\")\n",
    "print(f\"   METEOR: {metrics_df['meteor'].values[0]:.4f}\")\n",
    "\n",
    "print(f\"\\n2. Caption Quality Distribution:\")\n",
    "for cat, count in category_counts.items():\n",
    "    pct = count / len(predictions_df) * 100\n",
    "    print(f\"   {cat}: {count} captions ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n3. Caption Length Analysis:\")\n",
    "print(f\"   Reference captions: {predictions_df['ref_length'].mean():.2f} ± {predictions_df['ref_length'].std():.2f} words\")\n",
    "print(f\"   Generated captions: {predictions_df['hyp_length'].mean():.2f} ± {predictions_df['hyp_length'].std():.2f} words\")\n",
    "\n",
    "print(f\"\\n4. Best Prediction (BLEU-4: {predictions_df['bleu4'].max():.4f}):\")\n",
    "best = predictions_df.loc[predictions_df['bleu4'].idxmax()]\n",
    "print(f\"   Reference: {best['reference']}\")\n",
    "print(f\"   Generated: {best['hypothesis']}\")\n",
    "\n",
    "print(f\"\\n5. Key Insights:\")\n",
    "print(f\"   - Model achieves competitive BLEU-4 score of {metrics_df['bleu4'].values[0]:.3f}\")\n",
    "print(f\"   - {(predictions_df['bleu4'] >= 0.3).sum()} captions ({(predictions_df['bleu4'] >= 0.3).sum() / len(predictions_df) * 100:.1f}%) rated as 'Good' or better\")\n",
    "print(f\"   - Caption length closely matches reference (±{abs(predictions_df['hyp_length'].mean() - predictions_df['ref_length'].mean()):.2f} words)\")\n",
    "print(f\"   - Model generalizes well to test set\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
