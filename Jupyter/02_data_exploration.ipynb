{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration - Flickr8k Dataset\n",
    "\n",
    "Comprehensive exploration and analysis of the Flickr8k dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import pickle\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load cleaned captions\n",
    "data_dir = Path('../data/flickr8k')\n",
    "df = pd.read_csv(data_dir / 'processed/captions_clean.csv')\n",
    "\n",
    "# Load vocabulary\n",
    "with open(data_dir / 'processed/vocabulary.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"  Total captions: {len(df):,}\")\n",
    "print(f\"  Unique images: {df['image'].nunique():,}\")\n",
    "print(f\"  Vocabulary size: {len(vocab):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load splits\n",
    "train_df = pd.read_csv(data_dir / 'processed/train.csv')\n",
    "val_df = pd.read_csv(data_dir / 'processed/val.csv')\n",
    "test_df = pd.read_csv(data_dir / 'processed/test.csv')\n",
    "\n",
    "# Create summary table\n",
    "summary = pd.DataFrame({\n",
    "    'Split': ['Train', 'Validation', 'Test', 'Total'],\n",
    "    'Images': [\n",
    "        train_df['image'].nunique(),\n",
    "        val_df['image'].nunique(),\n",
    "        test_df['image'].nunique(),\n",
    "        df['image'].nunique()\n",
    "    ],\n",
    "    'Captions': [len(train_df), len(val_df), len(test_df), len(df)]\n",
    "})\n",
    "\n",
    "summary['Captions/Image'] = summary['Captions'] / summary['Images']\n",
    "summary['Percentage'] = (summary['Images'] / summary.loc[3, 'Images'] * 100).round(1)\n",
    "\n",
    "print(\"\\nDataset Split Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(summary.to_string(index=False))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize split distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Images per split\n",
    "splits = ['Train', 'Val', 'Test']\n",
    "image_counts = [train_df['image'].nunique(), val_df['image'].nunique(), test_df['image'].nunique()]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "ax1.bar(splits, image_counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Number of Images')\n",
    "ax1.set_title('Images per Split')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(image_counts):\n",
    "    ax1.text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(image_counts, labels=splits, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Distribution of Images')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Caption Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate caption lengths\n",
    "df['caption_length'] = df['caption'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Statistics\n",
    "stats = df['caption_length'].describe()\n",
    "print(\"\\nCaption Length Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Mean:   {stats['mean']:.2f} words\")\n",
    "print(f\"Median: {stats['50%']:.0f} words\")\n",
    "print(f\"Std:    {stats['std']:.2f} words\")\n",
    "print(f\"Min:    {stats['min']:.0f} words\")\n",
    "print(f\"Max:    {stats['max']:.0f} words\")\n",
    "print(f\"25%:    {stats['25%']:.0f} words\")\n",
    "print(f\"75%:    {stats['75%']:.0f} words\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize length distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(df['caption_length'], bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0, 0].axvline(df['caption_length'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[\"caption_length\"].mean():.1f}')\n",
    "axes[0, 0].axvline(df['caption_length'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {df[\"caption_length\"].median():.0f}')\n",
    "axes[0, 0].set_xlabel('Caption Length (words)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Caption Lengths')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[0, 1].boxplot(df['caption_length'], vert=False, patch_artist=True)\n",
    "axes[0, 1].set_xlabel('Caption Length (words)')\n",
    "axes[0, 1].set_title('Caption Length Distribution (Box Plot)')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Cumulative distribution\n",
    "sorted_lengths = np.sort(df['caption_length'])\n",
    "cumulative = np.arange(1, len(sorted_lengths) + 1) / len(sorted_lengths)\n",
    "axes[1, 0].plot(sorted_lengths, cumulative, linewidth=2)\n",
    "axes[1, 0].axhline(0.95, color='red', linestyle='--', label='95th percentile')\n",
    "axes[1, 0].set_xlabel('Caption Length (words)')\n",
    "axes[1, 0].set_ylabel('Cumulative Probability')\n",
    "axes[1, 0].set_title('Cumulative Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Length by split\n",
    "train_df['caption_length'] = train_df['caption'].apply(lambda x: len(x.split()))\n",
    "val_df['caption_length'] = val_df['caption'].apply(lambda x: len(x.split()))\n",
    "test_df['caption_length'] = test_df['caption'].apply(lambda x: len(x.split()))\n",
    "\n",
    "data_to_plot = [train_df['caption_length'], val_df['caption_length'], test_df['caption_length']]\n",
    "axes[1, 1].boxplot(data_to_plot, labels=['Train', 'Val', 'Test'], patch_artist=True)\n",
    "axes[1, 1].set_ylabel('Caption Length (words)')\n",
    "axes[1, 1].set_title('Caption Length by Split')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Count word frequencies\n",
    "all_words = []\n",
    "for caption in df['caption']:\n",
    "    all_words.extend(caption.split())\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "print(f\"\\nVocabulary Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total words (with repetition): {len(all_words):,}\")\n",
    "print(f\"Unique words: {len(word_freq):,}\")\n",
    "print(f\"Words in vocabulary: {len(vocab):,}\")\n",
    "print(f\"Vocabulary coverage: {len(vocab) / len(word_freq) * 100:.2f}%\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Top 30 most common words\n",
    "top_words = word_freq.most_common(30)\n",
    "\n",
    "words, counts = zip(*top_words)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.barh(range(len(words)), counts, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "plt.yticks(range(len(words)), words)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 30 Most Common Words')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Word cloud\n",
    "text = ' '.join(df['caption'])\n",
    "wordcloud = WordCloud(width=1200, height=600, background_color='white', colormap='viridis').generate(text)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of All Captions', fontsize=20, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Word frequency distribution\n",
    "freq_counts = Counter(word_freq.values())\n",
    "frequencies = sorted(freq_counts.keys())\n",
    "counts = [freq_counts[f] for f in frequencies]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(frequencies[:100], counts[:100], marker='o', linestyle='-', markersize=4)\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Word Frequency Distribution (First 100)')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.loglog(frequencies, counts, marker='o', linestyle='-', markersize=3)\n",
    "plt.xlabel('Word Frequency (log scale)')\n",
    "plt.ylabel('Number of Words (log scale)')\n",
    "plt.title('Word Frequency Distribution (Log-Log)')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample images with their captions\n",
    "import random\n",
    "\n",
    "images_dir = data_dir / 'Images'\n",
    "sample_images = random.sample(list(df['image'].unique()), 9)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_name in enumerate(sample_images):\n",
    "    img_path = images_dir / img_name\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    # Get one caption\n",
    "    caption = df[df['image'] == img_name]['caption'].values[0]\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].axis('off')\n",
    "    axes[idx].set_title(f\"{caption[:60]}...\", fontsize=9, wrap=True)\n",
    "\n",
    "plt.suptitle('Random Sample Images with Captions', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Caption Variation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Show variation in captions for same image\n",
    "sample_img = random.choice(list(df['image'].unique()))\n",
    "captions = df[df['image'] == sample_img]['caption'].values\n",
    "\n",
    "img_path = images_dir / sample_img\n",
    "img = Image.open(img_path)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Image: {sample_img}\", fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CAPTION VARIATIONS FOR SAME IMAGE:\")\n",
    "print(\"=\"*70)\n",
    "for i, caption in enumerate(captions, 1):\n",
    "    print(f\"{i}. {caption}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Insights\n",
    "\n",
    "Summary of findings from data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS FROM DATA EXPLORATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n1. Dataset Size:\")\n",
    "print(f\"   - {df['image'].nunique():,} unique images\")\n",
    "print(f\"   - {len(df):,} total captions\")\n",
    "print(f\"   - Average {len(df) / df['image'].nunique():.1f} captions per image\")\n",
    "\n",
    "print(f\"\\n2. Caption Characteristics:\")\n",
    "print(f\"   - Average length: {df['caption_length'].mean():.1f} words\")\n",
    "print(f\"   - Length range: {df['caption_length'].min()}-{df['caption_length'].max()} words\")\n",
    "print(f\"   - Most captions are {df['caption_length'].mode()[0]} words long\")\n",
    "\n",
    "print(f\"\\n3. Vocabulary:\")\n",
    "print(f\"   - {len(word_freq):,} unique words in dataset\")\n",
    "print(f\"   - {len(vocab):,} words in vocabulary (after filtering)\")\n",
    "print(f\"   - Top 3 words: {', '.join([w for w, _ in word_freq.most_common(3)])}\")\n",
    "\n",
    "print(f\"\\n4. Data Split:\")\n",
    "print(f\"   - Train: {train_df['image'].nunique():,} images ({train_df['image'].nunique() / df['image'].nunique() * 100:.1f}%)\")\n",
    "print(f\"   - Val: {val_df['image'].nunique():,} images ({val_df['image'].nunique() / df['image'].nunique() * 100:.1f}%)\")\n",
    "print(f\"   - Test: {test_df['image'].nunique():,} images ({test_df['image'].nunique() / df['image'].nunique() * 100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n5. Caption Diversity:\")\n",
    "print(f\"   - Each image has 5 different human-written captions\")\n",
    "print(f\"   - Captions show good variation in phrasing and focus\")\n",
    "print(f\"   - Vocabulary is rich and descriptive\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
